%% Version 6.1, 1 September 2021
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% amspaperV6.tex --  LaTeX-based instructional template paper for submissions to the 
% American Meteorological Society
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREAMBLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Start with one of the following:
% 1.5-SPACED VERSION FOR SUBMISSION TO THE AMS
\documentclass{ametsocV6.1}

% TWO-COLUMN JOURNAL PAGE LAYOUT---FOR AUTHOR USE ONLY
% \documentclass[twocol]{ametsocV6.1}




\title{Generative Machine Learning for Climate Modelling}


% \Author[affil]{given_name}{surname}

\authors{Philip Brohan, \aff{a} \correspondingauthor{Philip Brohan, philip.brohan@metoffice.gov.uk}}

\affiliation{\aff{a}{Met Office Hadley Centre, Exeter, UK}}



\abstract{Machine Learning (ML) is a very powerful tool for making generative models: models that can make detailed, complex output (often pictures) from a simple input specification. We can use this in science by making generative models of weather and climate --- by training ML models to output gridded fields of weather variables such as temperature and precipitation. Here we show that you can use a Variational AutoEncoder (VAE) to build a fast deep generative model linking physically-plausible weather fields to a complete, continuous, low-dimensional latent space. Such an ML model can be used in place of a General Circulation Model (GCM) for some purposes: We can do reanalysis by constraining the field output to match observations, or perturbation/attribution studies by using the ML model to estimate the effect of changing one variable on another (e.g. change in sea temperature on precipitation). The primary advantage of such an ML model over a traditional GCM is that it can be several orders of magnitude faster.}

\begin{document}
\maketitle


\statement
    Current climate models are powerful, but very slow and difficult to use, so we can't use them as much as we would like. This work shows how to use machine learning to make climate models that are very fast and easy to use.

\section{Introduction}
\label{Intro}

Traditional General Circulation Models (GCMs, \citet{phillips1956general}) are examples of {\em Generative Models} \citep{Generative}: a GCM will output a self-consistent and physically-plausible state of the weather. GCMs are extremely powerful and successful, but they do have important limitations: In particular they are {\em expensive} --- in both user effort and compute required --- so we have to limit the work we do with them.

Modern Machine Learning (ML) has produced some spectacularly-successful generative models: human faces \citep{StyleGAN2}, visual art \citep{Stable_Diffusion}, text chat \citep{ChatGPT}, ... And, in a short period, has produced several powerful techniques for building such models: autoregressive models \citep{oord2016pixel}, convolutional neural nets \citep{LeNet}, flow-based models \citep{dinh2017density}, transformers \citep{vaswani2017attention}, ... In addition, ML is very well-supported in both hardware and software, with powerful software libraries making it easy to use modern computer hardware very efficiently \citep{tensorflow2015-whitepaper}. So there is a strong and obvious temptation to use ML methods to do the work of a GCM --- to use ML to generate a self-consistent and physically-plausible state of the weather.

A key virtue of GCMs is that they are physically-explainable: They try to break down the climate system into subcomponents, and have a transparent model of each subcomponent based on established physical principles. (They don't quite succeed at this, some non-physical 'parameterizations' remain, but current GCMs are to a large extent physical and transparent.) An ML model gives up this advantage --- it will be a black box, with little or no physical basis. This is a substantial weakness in ML modelling: ML models will be more dependent on validation data, and we will have less confidence when using them outside the environment in which they have been trained, but it's not just a weakness, it does come with advantages.

A physical model has to include an adequate set of physical variables (you can't just model surface precipitation, you need three-dimensional humidity, evaporation, advection, nucleation, ...) and has to operate on the space and time scales where the physics is well understood, so time steps of minutes and grid sizes of tens of kilometres are commonly used. ML models have none of these restrictions --- if you are interested in country-scale monthly-average surface precipitation, you can build a model with only that.

Because of this, simple statistical models have been used alongside GCMs for decades \citep{MAGICC}. The innovation offered by modern ML is that we can now build {\it complex} statistical models --- we can retain the speed, simplicity and flexibility of statistical modelling, while producing the precise and detailed output that used to require a GCM.

\section{An ML Model Architecture for Climate Modelling}
\label{Architecture}

There is a rapidly-growing set of ML model architectures for weather prediction \citep{keisler2022forecasting,bi2022panguweather,nguyen2023climax}, but climate modelling is a different problem: We are much less interested in time-evolution, and much more interested in changing boundary conditions. So we need a different model architecture, with no time-prediction capability, but where we can manipulate the model state more generally. (Though we can add time-prediction to the model if desired, see section \ref{Architecture}\ref{extension}.)

ML has already demonstrated major success in image generation \citep{StyleGAN2,Stable_Diffusion}. A typical architecture for this represents a high-resolution image as a point in a low-dimensional 'latent space', and uses a convolutional neural net (CNN) to map the latent space representation into the real space image \citep{StyleGAN}. If we consider a climate field as an image --- with (say) pressure, temperature and precipitation corresponding to the red, green, and blue channels, we can use exactly the same model architecture to generate climate states (figure \ref{ML_model_structure}).

\begin{figure}[h]
\includegraphics[width=8.3cm]{ML_model_structure.pdf}
\caption{Conceptual model of an ML-based climate model.}
\label{ML_model_structure}
\end{figure}

This is an example of a representation model. We represent a particular climate state as a point in low dimensional latent space. To be useful, we want the latent space representation and CNN model to have several properties:
\begin{itemize}
    \item The representation should be complete --- any point in latent space should map onto a self-consistent and physically-plausible state of the weather.
    \item The representation should be continuous --- close points in latent space should map onto similar weather states.
    \item The model should be fast --- otherwise, we could just use a GCM.
    \item The model should be powerful --- the weather states it outputs should be detailed and realistic.
\end{itemize}

    
\subsection{A reduced space representation from a Variational Autoencoder}

We can create a model and representation with exactly these properties, using a Variational AutoEncoder (VAE, \citet{VAE_Intro}). An autoencoder is made up of two neural nets: an {\it encoder}, which maps an input field into an {\it embedding} in a low-dimensional latent space; and a {\it generator}, which maps the latent space embedding back into the input field \citep{Autoencoder}. The encoder and generator are trained together to make $\textrm{generator}(\textrm{encoder}(\textrm{input}))$ as close to the original input as possible. A {\it variational} autoencoder does two additional things: The encoder outputs an embedding distribution not a single point embedding, and the encoder::generator pair are trained not just on output quality, but also on how well the embeddings match a multi-variate unit normal distribution (figure \ref{VAE_structure}).

\begin{figure}[h]
\includegraphics[width=8.3cm]{VAE_structure.pdf}
\caption{Model training architecture based on a Variational AutoEncoder.}
\label{VAE_structure}
\end{figure}

When the VAE is trained, the generator function is exactly the model we require: Defining the generator as a deep CNN allows it to be powerful and fast, making the latent space representation a distribution makes it continuous, and constraining the latent space representation to be a unit normal makes it complete. A VAE, with deep neural nets, is a ridiculously powerful tool --- an automatic climate model factory.
     
\subsection{Data Assimilation by constraining the reduced space}
\label{section_DA}

It's not enough to have a model which can generate self-consistent and physically-plausible states of the weather. We also need to be able to control the model --- to be able to find the latent-space vector that corresponds to the weather state of interest. Essentially, this is the problem of data assimilation: If we have some kind of metric from our weather state of interest, how do we use this information to make our model produce this state?
The proposed ML architecture makes this problem easy to solve: We can search the latent space for the state with the desired property. Because the latent space is complete and continuous (by design), and the ML model is fast, it is an easy optimization problem - we can use gradient descent to find the latent space vector producing the real-space weather state with the desired properties \citep{MLDA}.

\subsection{Extension by linking additional generative models}
\label{extension}

We want the ML model to be as small as possible --- small models are easy to train and faster and simpler in operation. But we also want the model to be broadly useful, and so to include predictions of a wide range of weather and impacts variables. We resolve this dilemma by having a small core model, and an extension method: the model can be extended to another variable by training another ML model to map the core latent space state to the additional variable of interest (figure \ref{Extension_structure}).

\begin{figure}[h]
\includegraphics[width=8.3cm]{Extension_structure.pdf}
\caption{Process for extending the model to additional variables.}
\label{Extension_structure}
\end{figure}
     
So if the base model is of a few surface weather variables (for example), we could build an extension model to link in satellite radiances (and so calculate surface weather from satellite radiances, or vice versa). Or we could build an extension model to link in a time-series of some impacts variable (flood damage, hospital admissions, food prices, ...). 

This same method offers one easy way to add time dependence to this model: Build an extension model to link the latent space state at one point in time with the latent space state one day (month/year/hour) later (or earlier).


\section{An example model - monthly weather for the UK}
\label{Example}

The architecture described above is only any use if it actually works --- if it is possible to train such a model on the climate variables of interest. So the next step is to train and demonstrate such a model: We chose to model UK surface weather anomalies (SST, MSLP, air temperature, and precipitation) at a 1 km spatial resolution, and on a monthly timescale. 1 km is the native resolution of the training data used, and working on monthly averages makes it reasonable to neglect time dependence, simplifying the model.

It's not practical to describe every detail of a model in a paper, so as well as the description here, the full source code for the model (everything needed to reproduce these results), is available at the URL given below, in the {\it Data availability statement}.

\subsection{Training data}

The aim is to train the model on observational data (rather than GCM output). We relaxed this slightly in the interest of getting data that's easy to use. So the SST and MSLP data were taken from the 20th Century Reanalysis version 3 \citep{slivinski_20crv3.1}, and the air temperature and precipitation
were taken from HadUK-Grid \citep{HadUK-Grid}.

A challenge for ML modelling for climate, is that training data is often going to be limited. One reason to choose the UK as the domain to model is that it has some of the world's best observational data. Even so the training data available covers only the period 1884 to 2015 (1584 months) and every 11th month was reserved for validation. So only 1430 months of data were available for model training.

\subsection{ML model specification}

The ML model was implemented using the TensorFlow Machine Learning platform \citep{tensorflow2015-whitepaper}, and based on the Convolutional Variational Autoencoder in the TensorFlow documentation \citep{TF_CVAE}. The model is a deep convolutional variational autoencoder, with a 100-dimensional latent space, and five convolutional layers in both the encoder and the generator. For the details of the model, and the hyperparameters used, see the full source code, linked below.

\subsection{Model Training and Validation}

A challenge to training is that we are predicting four fields (SST, MSLP, T2M, and precipitation) with quite different statistical properties (precipitation is much more difficult to model than MSLP), but we have to combine the model fit to all four fields into a single training metric. We used the percentage of unexplained variance for each variable, summed across the variables, as the training metric, which worked well (figure \ref{training_progress}). (The automated weighting methods of the $\sigma$-VAE \citep{sigma-VAE} might be a better approach, but it was not used here.)



\begin{figure}[h]
\includegraphics[width=8.3cm]{training_progress.pdf}
\caption{Training progress: percentage of unexplained variance for each variable. Thick line shows validation months, thin line training months.}
\label{training_progress}
\end{figure}
    
Unsurprisingly, precipitation has the least skill of the four variables, but the model still explains 90\% of the variance. And there is almost no indication of overfitting. The model took about 6 hours to train on a single V100 GPU. Application of the trained model is much faster --- it takes almost no time or resources: a few seconds on laptop-type hardware.

We can see the power of the model by comparing the VAE output with the training input for a test month (figure \ref{model_validation}), or averaged over the UK for all the test months (figure \ref{model_validation_multi}).

\begin{figure}[h]
\includegraphics[width=8.3cm]{model_validation.pdf}
\caption{ML model validation for a test month (December 2004). Left column: Model input, middle column: Model output, right column: Input::Output scatter}
\label{model_validation}
\end{figure}


\begin{figure}[h]
\includegraphics[width=8.3cm]{model_validation_multi.pdf}
\caption{ML model validation for all sample months. UK regional average for each month in the validation dataset. Black lines: model input, red lines: model output, and Input::Output scatter.}
\label{model_validation_multi}
\end{figure}


\section{Applying the model}

The validation shows that this model, despite its simplicity, is capable of representing UK surface weather with accuracy and precision. But to be useful, a model needs to be able to do more than just reproduce its training data.


\subsection{An AMIP run}

The model includes four variables, and describes the covariance between those variables. So we can use it to reconstruct the full weather state vector from limited information. A typical example of such a requirement is an AMIP run, where the climate is reconstructed given just the SST. We can do such a run with this model by assimilating the observed SST for one month with the method described in section \ref{Architecture}\ref{section_DA}, to estimate the latent space state vector for the month, and then using the generative model to estimate the full weather state for that month.

Figure \ref{fit_to_SST} shows the time-series from such a calculation. The results are as expected: SST is perfect (it was assimilated), T2M has substantial skill, PRMSL and precipitation have small skill.

\begin{figure}[h]
\includegraphics[width=8.3cm]{fit_to_SST.pdf}
\caption{Regional mean time series from an AMIP run: Black lines: observed values, red lines: model output, and Observation::Model scatter.}
\label{fit_to_SST}
\end{figure}
    
\subsection{A reanalysis run}

The Twentieth Century Reanalysis has shown the power of assimilating just SST and pressure into a GCM \citep{slivinski_20crv3.1}. We can do such a reanalysis with the ML model too.

Figure \ref{fit_to_SST+PRMSL} shows the time-series from a run assimilating SST and PRMSL. Again the results are as expected: SST and PRMSL are perfect (assimilated), T2M and now also precipitation have substantial skill. This is a good illustration of the power of generative ML models, the complex relationship between temperature, pressure and precipitation is well captured by the ML model.

\begin{figure}[h]
\includegraphics[width=8.3cm]{fit_to_SST+PRMSL.pdf}
\caption{Regional mean time series from a reanalysis-type run assimilating SST and PRMSL: Black lines: observed values, red lines: model output, and Observation::Model scatter.}
\label{fit_to_SST+PRMSL}
\end{figure}

\subsection{Attribution}

Many of the important questions in climate research are about hypotheticals and changes --- we want our models to tell us what would happen if some particular change occurred. We can use the ML model for hypotheticals as well. For example, the wettest month in the UK records was October 1903, when SSTs were lower than they are at present --- would a meteorologically similar month in a climate with higher SSTs be even wetter?

To investigate this, we assimilate the observations for October 1903, to get the model latent space state vector. Then perturb the observed SSTs by adding 1$^\circ$C and find the change in state vector needed to match the changed SSTs, then the model output with the perturbed state vector will give the perturbed climate (figure \ref{perturbation_method}.)

\begin{figure}[h]
\includegraphics[width=8.3cm]{perturbation_method.pdf}
\caption{Process for estimating the effect of a climate perturbation (+1$^\circ$C) for an attribution study.}
\label{perturbation_method}
\end{figure}

The results of this perturbation experiment are shown in figure \ref{perturbation_validation}. October 1903 had very low MSLP and very high precipitation, with unremarkable temperatures. Perturbing the SST by +1$^\circ$C in the ML model produces a uniform increase in the air temperature, a similar increase in MSLP, and has little effect on the precipitation. 

\begin{figure}[h]
\includegraphics[width=8.3cm]{perturbation_validation.pdf}
\caption{Results of increasing SST by 1C. Left column: observed weather state for the record wet month of October 1903. Middle column: ML model prediction after perturbing SST by +1$^\circ$C. Right column: Scatter Model::observations.}
\label{perturbation_validation}
\end{figure}
        
This result is plausible --- the ML model has learned a relationship between the four variables from the training data, and used this relationship to predict the result of the SST perturbation. We can't validate the model result (there are no observations from the perturbed state), so we have to regard the result as speculative --- it would take much more testing to be confident that the ML model had learned a relationship that was appropriate in this case, but it does demonstrate that ML models can be used to do such experiments.

\subsection{Extending to climate services}

One virtue of ML Climate models is that they can treat all variables in the same way: We don't need to distinguish between physical variables (like temperature) and social variables (like life expectancy); the process for modelling them is exactly the same, and it's straightforward to combine very different variables into the same model.

So to add an impact variable to the ML model (perhaps as the basis for a climate service), the process is the same whatever variable we choose. In this example, we will use river flow (because observed river flow records are readily available from the National River Flow Archive \citep{NRFA}).

Specifically, we will add an extension model (section \ref{extension}) --- a 2-layer perceptron (details in attached source code) to predict the flow anomaly of the River Thames at Kingston \citep{NRFA-Kingston} from the climate model latent space vector. See figure \ref{services_river_flow}.

\begin{figure}[h]
\includegraphics[width=8.3cm]{services_river_flow.pdf}
\caption{Extending the model to river flow prediction. Climate observations (top right) are assimilated to find the latent space state vector for each month, and then a separate extension model is used to predict the river flow anomaly (bottom right) from the state vector. Black line: flow anomaly observations, red line: model output.}
\label{services_river_flow}
\end{figure}
         
This process works as expected. Note that a better river flow anomaly prediction could be produced with more modelling effort (for example with a deeper neural network or the use of weather data from previous months). But this version is sufficient to demonstrate the process.


\section{Conclusions}

For the past few decades, General Circulation Models have dominated modelling in weather and climate. Machine Learning provides a new way to build complex models, and so a challenge to the hegemony of the GCM. In weather forecasting there are already several ML models approaching the state of the art. 

Climate modelling is more complicated than weather forecasting. We are not just forecasting from a known state, we need to represent the full set of possible climate states, and find out how a climate state is affected by changes in boundary conditions. But we can still do the job with ML: The Variational AutoEncoder is a well-established ML method for image analysis, and it lets us represent possible climate states in a low-dimensional space of features, while retaining both the ability how the state changes as climate boundary conditions change, and to represent the complex spatial detail of real weather states.

Such ML-based climate models are not a replacement for a GCM - the two modelling approaches have very different strengths and limitations, but the ML models have the major advantage that they are orders of magnitude cheaper and easier to produce and to use. Such models will open up the large class of modelling questions where a GCM is too difficult, or too expensive, to use.

\acknowledgments
This work was supported by the Met Office Hadley Centre Climate Programme funded by BEIS, and by the UK-China Research \& Innovation Partnership Fund through the Met Office Climate Science for Service Partnership (CSSP) China as part of the Newton Fund.
This work used the Isambard UK National Tier-2 HPC Service operated by GW4 and the UK Met Office, and funded by EPSRC (EP/P020224/1).

\datastatement
The code needed to reproduce or build on this work is online at \url{https://brohan.org/ML_monthly_UK}. The repository includes tools to download the data required. 



%% REFERENCES

\bibliographystyle{ametsocV6}
\bibliography{MLCD}


\end{document}
